# -*- coding: utf-8 -*-
"""process_bank_churn

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1otzjRKE2AWzORsiKkgeguCeJOfO6dDWB
"""

# -*- coding: utf-8 -*-
from __future__ import annotations

from typing import Any, Dict, List, Optional, Sequence, Tuple

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline


def select_input_cols(
    df: pd.DataFrame,
    *,
    target_col: str,
    group_col: str,
    drop_cols: Sequence[str] = ("Surname",),
) -> List[str]:
    """Select feature columns to be used as model inputs."""
    excluded = {target_col, group_col, *drop_cols}
    return [c for c in df.columns if c not in excluded]


def split_data_by_customer_id_stratified(
    df: pd.DataFrame,
    *,
    group_col: str = "CustomerId",
    target_col: str = "Exited",
    test_size: float = 0.2,
    random_state: int = 42,
) -> Dict[str, pd.DataFrame]:
    """Split so that the same CustomerId never appears in both train and val.

    Stratification is done on per-customer target = max(Exited) within each CustomerId.
    """
    cust_y = (
        df.groupby(group_col, as_index=False)[target_col]
        .max()
        .rename(columns={target_col: "cust_target"})
    )

    train_ids, val_ids = train_test_split(
        cust_y[group_col],
        test_size=test_size,
        random_state=random_state,
        stratify=cust_y["cust_target"],
    )

    train_ids_set = set(train_ids)
    val_ids_set = set(val_ids)

    train_df = df[df[group_col].isin(train_ids_set)].copy()
    val_df = df[df[group_col].isin(val_ids_set)].copy()

    if not set(train_df[group_col]).isdisjoint(set(val_df[group_col])):
        raise ValueError("Group leakage: some CustomerId appears in both train and val.")

    return {"train": train_df, "val": val_df}


def create_inputs_targets(
    df_dict: Dict[str, pd.DataFrame],
    *,
    input_cols: Sequence[str],
    target_col: str,
) -> Dict[str, Any]:
    """Create inputs/targets for train and validation splits."""
    return {
        "train_inputs": df_dict["train"][list(input_cols)].copy(),
        "train_targets": df_dict["train"][target_col].copy(),
        "val_inputs": df_dict["val"][list(input_cols)].copy(),
        "val_targets": df_dict["val"][target_col].copy(),
    }


def get_numeric_categorical_cols(df: pd.DataFrame) -> Tuple[List[str], List[str]]:
    """Return numeric and categorical column names based on pandas dtypes."""
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = df.select_dtypes(include=["object"]).columns.tolist()
    return numeric_cols, categorical_cols


def build_numeric_pipeline(*, scaler_numeric: bool) -> Pipeline:
    """Create a numeric preprocessing pipeline: impute + optional scaling."""
    steps = [("imputer", SimpleImputer(strategy="mean"))]
    if scaler_numeric:
        steps.append(("scaler", StandardScaler()))
    return Pipeline(steps)


def build_onehot_encoder() -> OneHotEncoder:
    """Create a OneHotEncoder with safe defaults (handle_unknown='ignore')."""
    # Compatibility with older sklearn versions:
    try:
        return OneHotEncoder(handle_unknown="ignore", sparse_output=False)
    except TypeError:
        return OneHotEncoder(handle_unknown="ignore", sparse=False)


def fit_transform_numeric(
    X_train_df: pd.DataFrame,
    X_val_df: pd.DataFrame,
    *,
    numeric_cols: Sequence[str],
    scaler_numeric: bool,
) -> Tuple[np.ndarray, np.ndarray, Optional[Pipeline]]:
    """Fit numeric pipeline on train and transform both train/val."""
    if len(numeric_cols) == 0:
        n_train = len(X_train_df)
        n_val = len(X_val_df)
        return np.empty((n_train, 0)), np.empty((n_val, 0)), None

    num_pipe = build_numeric_pipeline(scaler_numeric=scaler_numeric)
    train_num = num_pipe.fit_transform(X_train_df[list(numeric_cols)])
    val_num = num_pipe.transform(X_val_df[list(numeric_cols)])
    return train_num, val_num, num_pipe


def fit_transform_categorical(
    X_train_df: pd.DataFrame,
    X_val_df: pd.DataFrame,
    *,
    categorical_cols: Sequence[str],
) -> Tuple[np.ndarray, np.ndarray, Optional[OneHotEncoder]]:
    """Fit OneHotEncoder on train and transform both train/val."""
    if len(categorical_cols) == 0:
        n_train = len(X_train_df)
        n_val = len(X_val_df)
        return np.empty((n_train, 0)), np.empty((n_val, 0)), None

    enc = build_onehot_encoder()
    train_cat = enc.fit_transform(X_train_df[list(categorical_cols)])
    val_cat = enc.transform(X_val_df[list(categorical_cols)])
    return train_cat, val_cat, enc


def preprocess_data(
    raw_df: pd.DataFrame,
    *,
    target_col: str = "Exited",
    group_col: str = "CustomerId",
    drop_cols: Sequence[str] = ("Surname",),
    test_size: float = 0.2,
    random_state: int = 42,
    scaler_numeric: bool = True,
) -> Tuple[
    np.ndarray,
    pd.Series,
    np.ndarray,
    pd.Series,
    List[str],
    Optional[Pipeline],
    Optional[OneHotEncoder],
]:
    """Full preprocessing: select columns, split, encode categorical, scale numeric.

    Returns:
        X_train, y_train, X_val, y_val, input_cols, numeric_pipeline, encoder
    """
    input_cols = select_input_cols(
        raw_df, target_col=target_col, group_col=group_col, drop_cols=drop_cols
    )

    split_dfs = split_data_by_customer_id_stratified(
        raw_df,
        group_col=group_col,
        target_col=target_col,
        test_size=test_size,
        random_state=random_state,
    )

    data = create_inputs_targets(split_dfs, input_cols=input_cols, target_col=target_col)
    train_inputs: pd.DataFrame = data["train_inputs"]
    val_inputs: pd.DataFrame = data["val_inputs"]
    train_targets: pd.Series = data["train_targets"]
    val_targets: pd.Series = data["val_targets"]

    numeric_cols, categorical_cols = get_numeric_categorical_cols(train_inputs)

    train_num, val_num, num_pipe = fit_transform_numeric(
        train_inputs, val_inputs, numeric_cols=numeric_cols, scaler_numeric=scaler_numeric
    )
    train_cat, val_cat, encoder = fit_transform_categorical(
        train_inputs, val_inputs, categorical_cols=categorical_cols
    )

    X_train = np.hstack([train_num, train_cat])
    X_val = np.hstack([val_num, val_cat])

    return X_train, train_targets, X_val, val_targets, input_cols, num_pipe, encoder


def preprocess_new_data(
    new_df: pd.DataFrame,
    *,
    input_cols: Sequence[str],
    scaler: Optional[Pipeline],
    encoder: Optional[OneHotEncoder],
) -> np.ndarray:
    """Preprocess new (unseen) data using already fitted scaler/encoder.

    Args:
        new_df: New raw data (e.g., test.csv loaded as DataFrame).
        input_cols: The same feature columns used during training.
        scaler: Fitted numeric pipeline (imputer + optional scaler) or None.
        encoder: Fitted OneHotEncoder or None.

    Returns:
        X_new: Numpy array with processed features in the same order as training.
    """
    X = new_df[list(input_cols)].copy()
    numeric_cols, categorical_cols = get_numeric_categorical_cols(X)

    # Numeric part
    if len(numeric_cols) == 0:
        X_num = np.empty((len(X), 0))
    else:
        if scaler is None:
            # If numeric pipeline wasn't used during training, pass raw numeric values
            X_num = X[list(numeric_cols)].to_numpy()
        else:
            X_num = scaler.transform(X[list(numeric_cols)])

    # Categorical part
    if len(categorical_cols) == 0:
        X_cat = np.empty((len(X), 0))
    else:
        if encoder is None:
            raise ValueError("Encoder is None but categorical columns exist in new_df.")
        X_cat = encoder.transform(X[list(categorical_cols)])

    return np.hstack([X_num, X_cat])